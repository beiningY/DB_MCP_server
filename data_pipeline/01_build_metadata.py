import os
import json
import datetime
from typing import List, Optional
from sqlalchemy import create_engine, inspect
from pydantic import BaseModel, Field
from openai import OpenAI
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
# 1. æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² (ç¤ºä¾‹ä¸º MySQL)
# æ ¼å¼: mysql+pymysql://user:password@host:port/dbname
DB_CONNECTION_STR = os.getenv("DB_URL", "mysql+pymysql://root:password@localhost:3306/singa_collection")

# 2. LLM é…ç½® (å…¼å®¹ OpenAI SDK çš„æ¨¡å‹ï¼Œå¦‚ DeepSeek, GPT-4, Qwen)
LLM_API_KEY = os.getenv("LLM_API_KEY", "sk-......")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1") # å¦‚æœç”¨å…¶ä»–æ¨¡å‹ï¼Œè¯·ä¿®æ”¹æ­¤å¤„
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4-t") # å»ºè®®ä½¿ç”¨æ™ºèƒ½ç¨‹åº¦è¾ƒé«˜çš„æ¨¡å‹

# 3. æäº¤ä¿¡æ¯é…ç½®
SUBMITTER_NAME = "Sarah"
OWNER_DEPT = "BIéƒ¨é—¨"

# ================= æ•°æ®æ¨¡å‹å®šä¹‰ (Pydantic) =================
# è¿™äº›æ¨¡å‹ç¡®ä¿è¾“å‡ºæ ¼å¼ä¸¥æ ¼ç¬¦åˆä½ çš„ JSON è¦æ±‚

class ColumnMeta(BaseModel):
    column_name: str
    data_type: str
    comment: str = Field(..., description="å­—æ®µä¸­æ–‡æ³¨é‡Šï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œè¯·æ ¹æ®å­—æ®µåæ¨æ–­")
    is_primary_key: bool = False
    related_table: Optional[str] = Field(None, description="æ¨æ–­å¯èƒ½å…³è”çš„è¡¨å")
    related_column: Optional[str] = Field(None, description="æ¨æ–­å¯èƒ½å…³è”çš„å­—æ®µ")
    dict_code: Optional[str] = Field(None, description="å¦‚æœæ˜¯æšä¸¾å€¼ï¼Œæ¨æ–­å­—å…¸ç¼–ç ")
    enum_values: Optional[List[str]] = Field(None, description="å¯èƒ½çš„æšä¸¾å€¼åˆ—è¡¨")
    is_pii: bool = Field(False, description="æ˜¯å¦åŒ…å«ä¸ªäººæ•æ„Ÿä¿¡æ¯(æ‰‹æœºå·,èº«ä»½è¯ç­‰)")

class TableMeta(BaseModel):
    table_name: str
    table_comment: str = Field(..., description="è¡¨ä¸­æ–‡æ³¨é‡Šï¼Œå¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œè¯·æ ¹æ®è¡¨åæ¨æ–­")
    business_domain: str = Field(..., description="æ ¹æ®è¡¨åæ¨æ–­ä¸šåŠ¡åŸŸï¼Œå¦‚: collection, marketing")
    granularity: str = Field(..., description="æ•°æ®ç²’åº¦ï¼Œå¦‚: per_call, per_user, per_transaction")
    owner: str = Field(..., description="æ ¹æ®ä¸šåŠ¡æ¨æ–­å¯èƒ½çš„å½’å±ç»„")
    columns: List[ColumnMeta]

# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def get_raw_schema(engine, table_name):
    """
    ä½¿ç”¨ SQLAlchemy è·å–æ•°æ®åº“ä¸­çš„åŸå§‹ç‰©ç†ç»“æ„
    """
    inspector = inspect(engine)
    columns = []
    
    # è·å–ä¸»é”®
    pk_constraint = inspector.get_pk_constraint(table_name)
    pks = pk_constraint.get('constrained_columns', [])
    
    # è·å–æ‰€æœ‰åˆ—
    for col in inspector.get_columns(table_name):
        columns.append({
            "column_name": col['name'],
            "data_type": str(col['type']),
            "comment": col.get('comment', ''), # æ•°æ®åº“åŸæœ¬çš„æ³¨é‡Šï¼Œå¯èƒ½ä¸ºç©º
            "is_primary_key": col['name'] in pks
        })
        
    return columns

def enrich_with_llm(client: OpenAI, table_name: str, raw_columns: list) -> TableMeta:
    """
    è°ƒç”¨å¤§æ¨¡å‹è¡¥å…¨è¯­ä¹‰ä¿¡æ¯
    """
    print(f"ğŸ¤–æ­£åœ¨è¯·æ±‚ AI è¡¥å…¨è¡¨: {table_name} ...")
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„é‡‘èç§‘æŠ€æ•°æ®æ¶æ„å¸ˆï¼Œç†Ÿæ‚‰ä¿¡è´·ã€å‚¬æ”¶ã€é£æ§ç­‰ä¸šåŠ¡ã€‚

è¯·æ ¹æ®æä¾›çš„æ•°æ®åº“è¡¨ç»“æ„ï¼Œè¡¥å…¨å…ƒæ•°æ®ä¿¡æ¯ã€‚è¿™æ˜¯å°å°¼çš„é‡‘èç§‘æŠ€å…¬å¸æ•°æ®åº“ã€‚

## ä¸¥æ ¼è¦æ±‚
1. **table_comment**: å¿…é¡»ç”¨ä¸­æ–‡æè¿°è¡¨çš„ä¸šåŠ¡å«ä¹‰ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨è¡¨åï¼ä¾‹å¦‚ "360_data_test_pass" â†’ "360é‡‘èé£æ§æµ‹è¯•é€šè¿‡è®¢å•è¡¨"
2. **business_domain**: è¯·å…ˆè‡ªè¡Œåˆ¤æ–­ï¼Œå¦‚æœä¸èƒ½åˆ¤æ–­çš„æ—¶å€™å¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
   - collection (å‚¬æ”¶)
   - marketing (è¥é”€)
   - risk (é£æ§)
   - user (ç”¨æˆ·)
   - order (è®¢å•)
   - payment (æ”¯ä»˜)
   - finance (è´¢åŠ¡)
   - credit (å¾ä¿¡)
   - operation (è¿è¥)
   - system (ç³»ç»Ÿ)
3. **granularity**: è¯·å…ˆè‡ªè¡Œåˆ¤æ–­ï¼Œå¦‚æœä¸èƒ½åˆ¤æ–­çš„æ—¶å€™å¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ï¼šper_user, per_order, per_call, per_day, per_record
4. **columns[].comment**: æ¯ä¸ªå­—æ®µå¿…é¡»æœ‰ä¸­æ–‡æ³¨é‡Šï¼Œä¸èƒ½ä¸ºç©ºæˆ–ä½¿ç”¨å­—æ®µå

## è¾“å…¥
è¡¨å: {table_name}
åˆ—ç»“æ„: {json.dumps(raw_columns, ensure_ascii=False)}

## è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼éµå¾ª)
{{
  "table_name": "{table_name}",
  "table_comment": "ä¸­æ–‡è¡¨æ³¨é‡Šï¼ˆå¿…å¡«ï¼‰",
  "business_domain": "ä»ä¸Šè¿°é€‰é¡¹ä¸­é€‰æ‹©",
  "granularity": "ä»ä¸Šè¿°é€‰é¡¹ä¸­é€‰æ‹©",
  "owner": "{OWNER_DEPT}",
  "columns": [
    {{
      "column_name": "å­—æ®µå",
      "data_type": "ç±»å‹",
      "comment": "ä¸­æ–‡æ³¨é‡Šï¼ˆå¿…å¡«ï¼‰",
      "is_primary_key": true/false,
      "is_pii": true/false,
      "related_table": "å…³è”è¡¨åæˆ–null",
      "related_column": "å…³è”å­—æ®µæˆ–null"
    }}
  ]
}}

ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦ markdown ä»£ç å—ã€‚"""

    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåªè¾“å‡º JSON çš„æ•°æ®åŠ©æ‰‹ã€‚è¾“å‡ºå¿…é¡»ç¬¦åˆæä¾›çš„ Schema ç»“æ„ã€‚"},
                {"role": "user", "content": prompt}
            ],
            # å¦‚æœä½ çš„ LLM æ”¯æŒ structured output (å¦‚æœ€æ–°çš„ OpenAI)ï¼Œå¯ä»¥å¼€å¯ä¸‹é¢è¿™è¡Œ
            # response_format={"type": "json_object"}, 
            temperature=0.1,
        )
        
        content = response.choices[0].message.content
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ markdown æ ‡è®°
        content = content.replace("```json", "").replace("```", "").strip()
        
        # è§£æ JSON
        data = json.loads(content)
        
        # å¤„ç† LLM å¯èƒ½è¿”å›çš„åµŒå¥—ç»“æ„
        # å¦‚: {'table_metadata': {...}} æˆ– {'table': {...}} æˆ– {'result': {...}}
        if 'columns' not in data:
            # å°è¯•ä»å¸¸è§çš„åµŒå¥—é”®ä¸­æå–
            for key in ['table_metadata', 'table', 'result', 'data', 'metadata']:
                if key in data and isinstance(data[key], dict):
                    data = data[key]
                    break
        
        # å¦‚æœä»ç„¶æ²¡æœ‰ columnsï¼Œä½¿ç”¨åŸå§‹åˆ—æ•°æ®
        if 'columns' not in data:
            print(f"  âš ï¸ LLM æœªè¿”å› columnsï¼Œä½¿ç”¨åŸå§‹ç»“æ„")
            data['columns'] = raw_columns
        
        # è¡¥å…… LLM å¯èƒ½é—æ¼æˆ–è¿”å›æ— æ•ˆå€¼çš„å­—æ®µ
        data['table_name'] = table_name
        data['owner'] = data.get('owner') or OWNER_DEPT
        
        # éªŒè¯ business_domain æ˜¯å¦åœ¨æœ‰æ•ˆåˆ—è¡¨ä¸­
        valid_domains = ['collection', 'marketing', 'risk', 'user', 'order', 'payment', 'finance', 'credit', 'operation', 'system']
        bd = data.get('business_domain', '').lower().strip()
        if bd not in valid_domains:
            # å°è¯•ä»è¿”å›å€¼ä¸­æå–å…³é”®è¯
            for domain in valid_domains:
                if domain in bd:
                    bd = domain
                    break
            else:
                bd = 'operation'  # é»˜è®¤ä¸ºè¿è¥
        data['business_domain'] = bd
        
        # éªŒè¯ granularity
        valid_granularities = ['per_user', 'per_order', 'per_call', 'per_day', 'per_record']
        gran = data.get('granularity', '').lower().strip()
        if gran not in valid_granularities:
            data['granularity'] = 'per_record'
        
        # éªŒè¯ table_comment ä¸èƒ½ä¸ºç©ºæˆ–ç­‰äºè¡¨å
        tc = data.get('table_comment', '').strip()
        if not tc or tc == table_name or tc.lower() == table_name.lower():
            # å°è¯•ç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„æè¿°
            data['table_comment'] = f"{table_name} ä¸šåŠ¡æ•°æ®è¡¨"
        
        # ç¡®ä¿ columns ä¸­çš„æ¯ä¸ªå­—æ®µéƒ½æœ‰å¿…è¦çš„å±æ€§
        for col in data.get('columns', []):
            col_name = col.get('column_name') or col.get('name', '')
            col['column_name'] = col_name
            col['data_type'] = col.get('data_type') or col.get('type', 'unknown')
            
            # éªŒè¯ comment ä¸èƒ½ä¸ºç©ºæˆ–ç­‰äºå­—æ®µå
            comment = col.get('comment', '').strip()
            if not comment or comment == col_name or comment.lower() == col_name.lower():
                col['comment'] = col_name  # è‡³å°‘ä¿ç•™å­—æ®µå
            
            col.setdefault('is_primary_key', False)
        
        # ç¡®ä¿ LLM è¿”å›çš„æ•°æ®è¡¥å…¨äº† columns é‡Œçš„å†…å®¹
        return TableMeta(**data)

    except Exception as e:
        print(f"âš ï¸ AI å¤„ç†è¡¨ {table_name} å¤±è´¥: {e}")
        # å¦‚æœ AI å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªä¿åº•çš„åŸºç¡€ç»“æ„
        fallback_columns = []
        for c in raw_columns:
            col_data = {
                'column_name': c.get('column_name', ''),
                'data_type': c.get('data_type', 'unknown'),
                'comment': c.get('comment') or c.get('column_name', ''),
                'is_primary_key': c.get('is_primary_key', False),
            }
            fallback_columns.append(ColumnMeta(**col_data))
        
        return TableMeta(
            table_name=table_name,
            table_comment=f"{table_name} (AIè§£æå¤±è´¥)",
            business_domain="unknown",
            granularity="unknown",
            owner=OWNER_DEPT,
            columns=fallback_columns
        )

# ================= ä¸»ç¨‹åº =================

def main():
    # 1. è¿æ¥æ•°æ®åº“
    try:
        engine = create_engine(DB_CONNECTION_STR)
        connection = engine.connect()
        inspector = inspect(engine)
        db_name = engine.url.database
        print(f"âœ… æˆåŠŸè¿æ¥æ•°æ®åº“: {db_name}")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return

    # 2. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

    # 3. è·å–æ‰€æœ‰è¡¨å
    all_tables = inspector.get_table_names()
    
    # é™åˆ¶å¤„ç†çš„è¡¨æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰ï¼Œè®¾ä¸º None è¡¨ç¤ºå¤„ç†å…¨éƒ¨
    MAX_TABLES = None  # æ”¹ä¸º None å¤„ç†å…¨éƒ¨è¡¨
    
    tables_to_process = all_tables[:MAX_TABLES] if MAX_TABLES else all_tables
    print(f"ğŸ“Š å‘ç° {len(all_tables)} å¼ è¡¨ï¼Œæœ¬æ¬¡å¤„ç† {len(tables_to_process)} å¼ ...")

    processed_tables = []

    # 4. éå†å¤„ç†æ¯å¼ è¡¨
    for i, table_name in enumerate(tables_to_process, 1):
        print(f"\n[{i}/{len(tables_to_process)}] å¤„ç†è¡¨: {table_name}")
        # 4.1 è·å–ç‰©ç†ç»“æ„
        raw_columns = get_raw_schema(engine, table_name)
        
        # 4.2 è°ƒç”¨ LLM è¿›è¡Œå¢å¼º
        # æç¤ºï¼šå¦‚æœè¡¨éå¸¸å¤šï¼Œå»ºè®®åœ¨è¿™é‡ŒåŠ ä¸ª sleep æˆ–è€…è¿›åº¦æ¡
        table_meta = enrich_with_llm(client, table_name, raw_columns)
        
        # è½¬ä¸ºå­—å…¸
        table_dict = table_meta.model_dump(exclude_none=True)
        processed_tables.append(table_dict)
        
        # æ‰“å°å¤„ç†ç»“æœ
        print(f"  âœ“ ä¸šåŠ¡åŸŸ: {table_dict.get('business_domain')}")
        print(f"  âœ“ æ³¨é‡Š: {table_dict.get('table_comment')}")
        print(f"  âœ“ å­—æ®µæ•°: {len(table_dict.get('columns', []))}")

    # 5. ç»„è£…æœ€ç»ˆ JSON
    final_output = {
        "database": db_name,
        "owner": OWNER_DEPT,
        "submitted_by": SUBMITTER_NAME,
        "submitted_at": datetime.datetime.now().strftime("%Y-%m-%d"),
        "tables": processed_tables
    }

    # æ‰“å°å®Œæ•´è¾“å‡ºé¢„è§ˆ
    print("\n" + "=" * 60)
    print("ğŸ“„ ç”Ÿæˆçš„å…ƒæ•°æ®é¢„è§ˆ:")
    print("=" * 60)
    print(json.dumps(final_output, ensure_ascii=False, indent=2))
    print("=" * 60)

    # 6. ä¿å­˜æ–‡ä»¶
    filename = f"../metadata/{db_name}_metadata.json"
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜ä¸º: {filename}")

if __name__ == "__main__":
    main()