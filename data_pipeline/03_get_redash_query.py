import os
import json
import re
import requests
import sqlglot
from sqlglot import exp
from dotenv import load_dotenv
import time
import urllib3

# ç¦ç”¨ SSL è­¦å‘Šï¼ˆå†…ç½‘è¯ä¹¦å¯èƒ½ä¸å—ä¿¡ä»»ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ============ ç½‘ç»œé…ç½® ============
# Redash èµ°æœ¬åœ°ç½‘ç»œï¼Œä¸ä½¿ç”¨ä»£ç†
# å¦‚æœä¹‹å‰è®¾ç½®äº†ä»£ç†ç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œæ¸…é™¤
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
print("âœ“ ä½¿ç”¨æœ¬åœ°ç½‘ç»œè¿æ¥ Redashï¼ˆæ— ä»£ç†ï¼‰")
# =================================

REDASH_HOST = os.getenv("REDASH_URL")
API_KEY = os.getenv("REDASH_API_KEY")
HEADERS = {"Authorization": f"Key {API_KEY}"}

# SSL éªŒè¯è®¾ç½®ï¼ˆå†…ç½‘å¯èƒ½éœ€è¦å…³é—­ï¼‰
VERIFY_SSL = False

# ================= å·¥å…·å‡½æ•° =================

def get_all_items(endpoint):
    """
    å¤„ç†åˆ†é¡µï¼Œè·å– Redash æ‰€æœ‰æ•°æ® (Dashboard æˆ– Queries)
    """
    items = []
    page = 1
    page_size = 100
    
    print(f"ğŸ“¡ å¼€å§‹è·å– {endpoint} ...")
    
    while True:
        try:
            url = f"{REDASH_HOST}/api/{endpoint}?page={page}&page_size={page_size}"
            response = requests.get(url, headers=HEADERS, verify=VERIFY_SSL, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # Redash API çš„è¿”å›ç»“æ„é€šå¸¸æ˜¯ {'results': [...], 'count': ...}
            current_results = data.get('results', [])
            if not current_results:
                break
                
            items.extend(current_results)
            print(f"   - ç¬¬ {page} é¡µè·å–æˆåŠŸï¼Œå½“å‰æ€»æ•°: {len(items)}")
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            if len(current_results) < page_size:
                break
                
            page += 1
            # é¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
            time.sleep(0.2)
            
        except Exception as e:
            print(f"âŒ è·å–ç¬¬ {page} é¡µå¤±è´¥: {e}")
            break
            
    return items

def extract_tables_regex_fallback(sql_text):
    """
    æ­£åˆ™å…œåº•ï¼šå½“ sqlglot è§£æå¤±è´¥æ—¶ï¼Œç”¨æ­£åˆ™æå–è¡¨å
    """
    tables = set()
    
    # åŒ¹é… FROM/JOIN/INTO/UPDATE åçš„è¡¨åï¼ˆæ”¯æŒ schema.table æ ¼å¼ï¼‰
    patterns = [
        r'\bFROM\s+([a-zA-Z_][\w]*(?:\.[a-zA-Z_][\w]*)?)',
        r'\bJOIN\s+([a-zA-Z_][\w]*(?:\.[a-zA-Z_][\w]*)?)',
        r'\bINTO\s+([a-zA-Z_][\w]*(?:\.[a-zA-Z_][\w]*)?)',
        r'\bUPDATE\s+([a-zA-Z_][\w]*(?:\.[a-zA-Z_][\w]*)?)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, sql_text, re.IGNORECASE)
        tables.update(matches)
    
    # è¿‡æ»¤æ‰ SQL å…³é”®å­—
    keywords = {'SELECT', 'SET', 'VALUES', 'NULL', 'TRUE', 'FALSE', 'AS', 'ON', 'AND', 'OR', 'NOT'}
    tables = {t for t in tables if t.upper() not in keywords}
    
    return list(tables)


def extract_tables_from_sql(sql_text):
    """
    ä½¿ç”¨ sqlglot è§£æ SQLï¼Œæå–ç”¨åˆ°çš„è¡¨åï¼ˆå¢å¼ºç‰ˆï¼‰
    """
    if not sql_text:
        return []
    
    try:
        clean_sql = sql_text
        
        # 1. å°† {{ param }} æ›¿æ¢ä¸ºå¸¦å¼•å·çš„å ä½ç¬¦ï¼ˆå¤„ç†å‚æ•°ä¸­çš„ç©ºæ ¼ï¼‰
        def replace_param(match):
            param_name = match.group(1).strip().replace(' ', '_').replace('.', '_')
            return f"'__PARAM_{param_name}__'"
        clean_sql = re.sub(r'\{\{\s*([^}]+?)\s*\}\}', replace_param, clean_sql)
        
        # 2. å°† MySQL çš„ # æ³¨é‡Šè½¬æ¢ä¸ºæ ‡å‡† -- æ³¨é‡Š
        clean_sql = re.sub(r'#(.*)$', r'-- \1', clean_sql, flags=re.MULTILINE)
        
        # 3. ç§»é™¤ç”¨æˆ·å˜é‡èµ‹å€¼ @var:=valueï¼ˆMySQL ç‰¹æœ‰è¯­æ³•ï¼‰
        clean_sql = re.sub(r'@\w+\s*:=\s*', '', clean_sql)
        
        # 4. ä½¿ç”¨ sqlglot.parse è§£æå¤šæ¡è¯­å¥
        parsed_statements = sqlglot.parse(clean_sql, dialect="mysql")
        
        tables = set()
        for statement in parsed_statements:
            if statement:
                for t in statement.find_all(exp.Table):
                    table_name = t.sql()
                    if table_name and not table_name.startswith("'__PARAM_"):
                        tables.add(table_name)
        
        return list(tables)
        
    except Exception as e:
        # sqlglot è§£æå¤±è´¥æ—¶ï¼Œä½¿ç”¨æ­£åˆ™å…œåº•
        return extract_tables_regex_fallback(sql_text)

# ================= æ ¸å¿ƒå¤„ç†é€»è¾‘ =================

def main():
    if not REDASH_HOST or not API_KEY:
        print("âŒ è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® REDASH_URL å’Œ REDASH_API_KEY")
        return

    # 1. è·å–æ‰€æœ‰ Dashboard
    # æˆ‘ä»¬éœ€è¦ Dashboard çš„è¯¦ç»†ä¿¡æ¯æ‰èƒ½çŸ¥é“å®ƒåŒ…å«å“ªäº› Widget (Query)
    # åˆ—è¡¨æ¥å£é€šå¸¸ä¸ç»™ widgets è¯¦æƒ…ï¼Œæ‰€ä»¥å…ˆæ‹¿åˆ—è¡¨ï¼Œå†ç”± Query åæŸ¥æˆ–è€…åç»­å¢å¼º
    # ä½†ä¸ºäº†ç»™ LightRAG æœ€å¥½çš„è¯­æ–™ï¼Œæˆ‘ä»¬å°è¯•è·å– Dashboard -> Query çš„å…³ç³»
    
    dashboards = get_all_items("dashboards")
    queries = get_all_items("queries")

    # ä¸ºäº†å¿«é€ŸæŸ¥æ‰¾ Dashboard ä¿¡æ¯ï¼Œå»ºç«‹ä¸€ä¸ªæ˜ å°„
    # æ³¨æ„ï¼šRedash çš„ Query API è¿”å›ä¸­å¹¶æ²¡æœ‰ç›´æ¥åŒ…å« "å±äºå“ªä¸ª Dashboard"
    # å…³ç³»å­˜å‚¨åœ¨ Dashboard å¯¹è±¡é‡Œçš„ widgets åˆ—è¡¨é‡Œ
    
    query_usage_map = {} # query_id -> [dashboard_names]
    
    print("\nğŸ” æ­£åœ¨åˆ†æ Dashboard ä¸ Query çš„å…³è”...")
    for dash in dashboards:
        # è·å– Dashboard è¯¦æƒ…ï¼ˆå› ä¸ºåˆ—è¡¨é‡Œé€šå¸¸æ²¡æœ‰ widgets å­—æ®µï¼‰
        try:
            slug = dash.get('slug')
            resp = requests.get(f"{REDASH_HOST}/api/dashboards/{slug}", headers=HEADERS, verify=VERIFY_SSL, timeout=30)
            if resp.status_code == 200:
                dash_detail = resp.json()
                dash_name = dash_detail.get('name', 'Unknown Dashboard')
                
                # éå† widgets æ‰¾ query
                for widget in dash_detail.get('widgets', []):
                    visualization = widget.get('visualization')
                    if visualization and 'query' in visualization:
                        q_id = visualization['query'].get('id')
                        if q_id:
                            if q_id not in query_usage_map:
                                query_usage_map[q_id] = []
                            query_usage_map[q_id].append(dash_name)
            time.sleep(0.1)
        except Exception as e:
            print(f"âš ï¸ è·å– Dashboard {slug} è¯¦æƒ…å¤±è´¥: {e}")

    # 2. æ„å»ºç»“æ„åŒ–çš„ Query åˆ—è¡¨
    query_list = []
    
    print("\nğŸ“ æ­£åœ¨å¤„ç† Query æ•°æ®...")
    
    for q in queries:
        q_id = q.get('id')
        q_name = q.get('name', 'æœªå‘½åæŸ¥è¯¢')
        description = q.get('description') or ""
        sql = q.get('query', '')
        created_at = q.get('created_at', '')
        updated_at = q.get('updated_at', '')
        user = q.get('user', {})
        user_name = user.get('name', '') if user else ''
        
        # æå–è¡¨å
        tables = extract_tables_from_sql(sql)
        
        # è·å–æ‰€å± Dashboard
        related_dashboards = query_usage_map.get(q_id, [])
        
        # æ„å»ºç»“æ„åŒ–æ•°æ®
        query_data = {
            "id": q_id,
            "name": q_name,
            "description": description,
            "sql": sql,
            "tables_used": tables,
            "related_dashboards": related_dashboards,
            "created_by": user_name,
            "created_at": created_at,
            "updated_at": updated_at
        }
        
        query_list.append(query_data)

    # 3. è¾“å‡ºä¿å­˜ä¸º JSON
    output_file = "../metadata/redash_queries.json"
    
    result = {
        "total_queries": len(query_list),
        "total_dashboards": len(dashboards),
        "queries": query_list
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
            
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"å…±æå– {len(query_list)} ä¸ªæŸ¥è¯¢ï¼Œå…³è” {len(dashboards)} ä¸ªä»ªè¡¨æ¿")
    print(f"æ•°æ®å·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    main()